# ç¬¬äºŒç« ï¼šRAG çŸ¥è¯†åº“ç³»ç»Ÿ

## ç†è®ºè®²è§£

### 2.1 RAG æŠ€æœ¯æ¦‚è¿°ä¸æ ¸å¿ƒä»·å€¼

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼Œç®€ç§° RAGï¼‰æ˜¯å½“å‰å¤§è¯­è¨€æ¨¡å‹åº”ç”¨é¢†åŸŸæœ€é‡è¦çš„æŠ€æœ¯èŒƒå¼ä¹‹ä¸€ã€‚å®ƒå°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆï¼Œé€šè¿‡å…ˆæ£€ç´¢åç”Ÿæˆçš„æ–¹å¼ï¼Œè®©å¤§æ¨¡å‹èƒ½å¤ŸåŸºäºç‰¹å®šçŸ¥è¯†åº“å›ç­”é—®é¢˜ï¼Œä»è€Œæœ‰æ•ˆè§£å†³å¤§æ¨¡å‹çŸ¥è¯†è¿‡æ—¶ã€å¹»è§‰é—®é¢˜ä»¥åŠé¢†åŸŸçŸ¥è¯†ä¸è¶³ç­‰å›ºæœ‰ç¼ºé™·ã€‚RAG æŠ€æœ¯çš„æ ¸å¿ƒä»·å€¼ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œå®ƒèƒ½å¤Ÿè®©å¤§æ¨¡å‹è®¿é—®æœ€æ–°çš„ã€ç‰¹å®šé¢†åŸŸçš„ä¿¡æ¯ï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹æœ¬èº«è¿›è¡Œé‡æ–°è®­ç»ƒï¼›å…¶æ¬¡ï¼Œé€šè¿‡å¼•ç”¨å¤–éƒ¨çŸ¥è¯†æºï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ¨¡å‹äº§ç”Ÿå¹»è§‰çš„é£é™©ï¼›ç¬¬ä¸‰ï¼Œä¼ä¸šå¯ä»¥å°†ç§æœ‰æ•°æ®å®‰å…¨åœ°ç”¨äºå¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œæ— éœ€å°†æ•æ„Ÿæ•°æ®æš´éœ²ç»™æ¨¡å‹è®­ç»ƒæ–¹ã€‚

QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ é¡¹ç›®ä¸­çš„ RAG ç³»ç»Ÿæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–ä»æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–å­˜å‚¨åˆ°æ£€ç´¢ç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚è¿™ä¸ªç³»ç»Ÿçš„è®¾è®¡å……åˆ†è€ƒè™‘äº†æ•™å­¦å’Œå®æˆ˜çš„å¹³è¡¡ï¼Œæ—¢æœ‰æ·±å…¥çš„æŠ€æœ¯åŸç†è®²è§£ï¼Œåˆæœ‰å¯ç›´æ¥è¿è¡Œçš„ä»£ç ç¤ºä¾‹ã€‚é€šè¿‡å­¦ä¹ è¿™ä¸ªç³»ç»Ÿï¼Œå¼€å‘è€…ä¸ä»…èƒ½å¤ŸæŒæ¡ RAG çš„æ ¸å¿ƒæ¦‚å¿µï¼Œè¿˜èƒ½ç†è§£å®é™…å·¥ç¨‹ä¸­çš„å„ç§ä¼˜åŒ–ç­–ç•¥å’Œæœ€ä½³å®è·µã€‚

RAG æŠ€æœ¯çš„åº”ç”¨åœºæ™¯éå¸¸å¹¿æ³›ã€‚åœ¨ä¼ä¸šçŸ¥è¯†ç®¡ç†é¢†åŸŸï¼ŒRAG å¯ä»¥æ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œè®©å‘˜å·¥èƒ½å¤Ÿè‡ªç„¶è¯­è¨€æŸ¥è¯¢å…¬å¸å†…éƒ¨çš„æ–‡æ¡£ã€åˆ¶åº¦å’Œæµç¨‹ã€‚åœ¨æ•™è‚²åŸ¹è®­é¢†åŸŸï¼ŒRAG å¯ä»¥åˆ›å»ºåŸºäºæ•™æå’Œå‚è€ƒèµ„æ–™çš„æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿã€‚åœ¨å®¢æˆ·æ”¯æŒé¢†åŸŸï¼ŒRAG å¯ä»¥æ„å»ºèƒ½å¤Ÿå›ç­”äº§å“ç›¸å…³é—®é¢˜çš„å®¢æœæœºå™¨äººã€‚è¿™äº›åœºæ™¯çš„å…±åŒç‰¹ç‚¹æ˜¯ï¼šéœ€è¦åŸºäºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å†…å®¹æä¾›å‡†ç¡®ã€å¯é çš„å›ç­”ï¼Œè€Œé€šç”¨çš„å¤§è¯­è¨€æ¨¡å‹å¾€å¾€æ— æ³•æ»¡è¶³è¿™ç§éœ€æ±‚ã€‚

### 2.2 æ–‡æ¡£å¤„ç†ä¸æ–‡æœ¬åˆ†å—åŸç†

æ–‡æ¡£å¤„ç†æ˜¯ RAG ç³»ç»Ÿçš„ç¬¬ä¸€ä¸ªå…³é”®ç¯èŠ‚ï¼Œå®ƒå†³å®šäº†åç»­æ£€ç´¢å’Œç”Ÿæˆçš„è´¨é‡ã€‚é«˜è´¨é‡çš„æ–‡æ¡£å¤„ç†éœ€è¦è§£å†³å‡ ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ä»ä¸åŒæ ¼å¼çš„æ–‡æ¡£ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ã€å¦‚ä½•å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆé€‚åˆæ£€ç´¢çš„å°å—ã€å¦‚ä½•ä¿ç•™æ–‡æ¡£çš„ç»“æ„ä¿¡æ¯å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ é¡¹ç›®æ”¯æŒçš„æ–‡æ¡£æ ¼å¼åŒ…æ‹¬ PDFã€Markdownã€çº¯æ–‡æœ¬ã€HTML å’Œ JSONï¼Œæ¯ç§æ ¼å¼éƒ½éœ€è¦ä¸“é—¨çš„è§£æç­–ç•¥ã€‚

PDF æ–‡æ¡£æ˜¯å®é™…åº”ç”¨ä¸­æœ€å¸¸è§çš„æ–‡æ¡£æ ¼å¼ä¹‹ä¸€ï¼Œä½†ä¹Ÿæ˜¯å¤„ç†éš¾åº¦æœ€å¤§çš„æ ¼å¼ã€‚PDF çš„æœ¬è´¨æ˜¯é¡µé¢å¸ƒå±€æè¿°æ–‡ä»¶ï¼Œè€Œéç»“æ„åŒ–æ–‡æœ¬ï¼ŒåŒä¸€é¡µé¢çš„æ–‡å­—å¯èƒ½åœ¨æºä»£ç ä¸­åˆ†æ•£åœ¨ä¸åŒä½ç½®ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ ä½¿ç”¨ pypdf åº“è¿›è¡Œ PDF æ–‡æœ¬æå–ï¼Œæå–åçš„æ–‡æœ¬éœ€è¦ç»è¿‡æ¸…æ´—å’Œé‡ç»„ï¼Œå»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œæ— æ„ä¹‰çš„ç¬¦å·ã€‚å¯¹äºåŒ…å«è¡¨æ ¼æˆ–å›¾ç‰‡çš„ PDFï¼Œç›®å‰ä¸»è¦ä¾èµ– OCR æŠ€æœ¯è¿›è¡Œè¯†åˆ«ï¼Œä½†æ•ˆæœä»æœ‰æå‡ç©ºé—´ã€‚

Markdown æ–‡æ¡£çš„ç»“æ„ç›¸å¯¹æ¸…æ™°ï¼Œæ ‡é¢˜å±‚çº§ã€ä»£ç å—ã€åˆ—è¡¨ç­‰å…ƒç´ éƒ½æœ‰æ˜ç¡®çš„æ ‡è®°ã€‚å¤„ç† Markdown æ—¶ï¼Œå¯ä»¥ä¿ç•™å…¶æ ‡é¢˜ç»“æ„ä¿¡æ¯ï¼Œç”¨äºåç»­çš„æ–‡æœ¬åˆ†å—ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æŒ‰ç…§æ ‡é¢˜å±‚çº§è¿›è¡Œåˆ†å—ï¼Œæ¯ä¸ªå—ä»£è¡¨ä¸€ä¸ªå°èŠ‚çš„å†…å®¹ï¼Œè¿™æ ·èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒä¸»é¢˜çš„ä¸€è‡´æ€§ã€‚HTML æ–‡æ¡£çš„å¤„ç†ç±»ä¼¼ï¼Œéœ€è¦ä½¿ç”¨ BeautifulSoup ç­‰åº“è§£æ DOM ç»“æ„ï¼Œæå–æ­£æ–‡å†…å®¹ï¼ŒåŒæ—¶å»é™¤å¯¼èˆªæ ã€å¹¿å‘Šç­‰æ— å…³ä¿¡æ¯ã€‚

æ–‡æœ¬åˆ†å—ï¼ˆChunkingï¼‰æ˜¯ RAG ç³»ç»Ÿä¸­æœ€é‡è¦çš„ç¯èŠ‚ä¹‹ä¸€ã€‚åˆ†å—ç­–ç•¥çš„é€‰æ‹©ç›´æ¥å½±å“æ£€ç´¢æ•ˆæœï¼šå¦‚æœå—å¤ªå¤§ï¼Œå¯èƒ½åŒ…å«å¤ªå¤šæ— å…³ä¿¡æ¯ï¼Œå¢åŠ æ¨¡å‹çš„å¤„ç†è´Ÿæ‹…ï¼›å¦‚æœå—å¤ªå°ï¼Œå¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´å›ç­”ä¸å®Œæ•´ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ é‡‡ç”¨é€’å½’å­—ç¬¦åˆ†å—ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä½†éå¸¸æœ‰æ•ˆçš„åˆ†å—æ–¹æ³•ã€‚å®ƒé¦–å…ˆå°è¯•æŒ‰æ®µè½åˆ†å‰²ï¼Œå¦‚æœæ®µè½å¤ªå¤§åˆ™æŒ‰å¥å­åˆ†å‰²ï¼Œå¦‚æœå¥å­è¿˜æ˜¯å¤ªå¤§åˆ™æŒ‰å•è¯åˆ†å‰²ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„åˆ†å—æ–¹å¼èƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶æ§åˆ¶å—çš„å¤§å°ã€‚

åˆ†å—å‚æ•°çš„é€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´ã€‚`chunk_size` æ§åˆ¶æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤å€¼é€šå¸¸åœ¨ 500 åˆ° 1000 ä¹‹é—´ã€‚`chunk_overlap` æ§åˆ¶ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œè®¾ç½®é‡å å¯ä»¥ç¡®ä¿é‡è¦ä¿¡æ¯ä¸ä¼šå› ä¸ºåˆ†å—è¾¹ç•Œè€Œè¢«åˆ‡æ–­ã€‚é‡å å¤§å°é€šå¸¸è®¾ç½®ä¸ºå—å¤§å°çš„ 10% åˆ° 20%ã€‚åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œè¿˜å¯ä»¥é‡‡ç”¨æ»‘åŠ¨çª—å£ã€è¯­ä¹‰åˆ†å—ç­‰æ›´é«˜çº§çš„åˆ†å—ç­–ç•¥ã€‚

### 2.3 å‘é‡åŒ–ä¸å‘é‡æ£€ç´¢æŠ€æœ¯

å‘é‡åŒ–æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹ï¼Œæ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å‘é‡ï¼ˆEmbeddingï¼‰èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­å½¼æ­¤æ¥è¿‘ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ ä½¿ç”¨ OpenAI çš„ Embedding APIï¼Œç›®å‰æ”¯æŒ text-embedding-3-small å’Œ text-embedding-3-large ä¸¤ä¸ªæ¨¡å‹ã€‚text-embedding-3-small åœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´å–å¾—äº†è¾ƒå¥½çš„å¹³è¡¡ï¼Œæ˜¯å¤§å¤šæ•°åœºæ™¯çš„é¦–é€‰ã€‚

Embedding æ¨¡å‹çš„é€‰å‹éœ€è¦è€ƒè™‘å‡ ä¸ªå› ç´ ï¼šé¦–å…ˆæ˜¯ç»´åº¦æ•°ï¼Œé€šå¸¸ Embedding çš„ç»´åº¦è¶Šé«˜ï¼Œèƒ½è¡¨è¾¾çš„è¯­ä¹‰ä¿¡æ¯è¶Šä¸°å¯Œï¼Œä½†ä¹Ÿä¼šå¢åŠ å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼›å…¶æ¬¡æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä¸åŒæ¨¡å‹æ”¯æŒçš„æœ€å¤§è¾“å…¥é•¿åº¦ä¸åŒï¼›ç¬¬ä¸‰æ˜¯ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ï¼ŒæŸäº› Embedding æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦ã€æ³•å¾‹ï¼‰å¯èƒ½æœ‰æ›´å¥½çš„è¡¨ç°ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ é‡‡ç”¨çš„ text-embedding-3-small æ”¯æŒ 8191 ä¸ª token çš„è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œè¾“å‡º 1536 ç»´çš„å‘é‡ã€‚

å‘é‡æ£€ç´¢æ˜¯ RAG ç³»ç»Ÿçš„å¦ä¸€ä¸ªæ ¸å¿ƒæŠ€æœ¯ã€‚åœ¨æµ·é‡çš„å‘é‡ä¸­å¿«é€Ÿæ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ï¼Œéœ€è¦ä¸“é—¨çš„å‘é‡æ•°æ®åº“å’Œæ£€ç´¢ç®—æ³•ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ é»˜è®¤ä½¿ç”¨ FAISSï¼ˆFacebook AI Similarity Searchï¼‰ä½œä¸ºå‘é‡å­˜å‚¨æ–¹æ¡ˆã€‚FAISS æä¾›äº†å¤šç§ç´¢å¼•ç±»å‹ï¼ŒåŒ…æ‹¬ Flatï¼ˆç²¾ç¡®æ£€ç´¢ï¼‰ã€IVFï¼ˆå€’æ’æ–‡ä»¶ç´¢å¼•ï¼‰ã€HNSWï¼ˆå±‚æ¬¡å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼‰ç­‰ã€‚åœ¨å°è§„æ¨¡æ•°æ®ï¼ˆå‡ åƒåˆ°å‡ ä¸‡æ¡ï¼‰åœºæ™¯ä¸‹ï¼ŒFlat ç´¢å¼•å·²ç»è¶³å¤Ÿä½¿ç”¨ï¼Œç²¾ç¡®åº¦é«˜ä¸”æ²¡æœ‰ç´¢å¼•æ„å»ºå¼€é”€ã€‚

å‘é‡æ£€ç´¢çš„æ ¸å¿ƒæ˜¯ç›¸ä¼¼åº¦è®¡ç®—ã€‚å¸¸ç”¨çš„ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼åŒ…æ‹¬æ¬§æ°è·ç¦»ã€ä½™å¼¦ç›¸ä¼¼åº¦å’Œç‚¹ç§¯ã€‚ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡çš„æ˜¯å‘é‡æ–¹å‘çš„ç›¸ä¼¼æ€§ï¼Œå–å€¼èŒƒå›´åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼ã€‚åœ¨æ–‡æœ¬æ£€ç´¢åœºæ™¯ï¼Œä½™å¼¦ç›¸ä¼¼åº¦æ˜¯æœ€å¸¸ç”¨çš„åº¦é‡æ–¹å¼ï¼Œå› ä¸ºå®ƒå¯¹å‘é‡çš„é•¿åº¦ä¸æ•æ„Ÿï¼Œæ›´å…³æ³¨è¯­ä¹‰æ–¹å‘çš„ä¸€è‡´æ€§ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ åœ¨æ£€ç´¢æ—¶è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„ K ä¸ªæ–‡æ¡£ï¼ŒK çš„å€¼é€šå¸¸åœ¨ 3 åˆ° 10 ä¹‹é—´ï¼Œéœ€è¦æ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´ã€‚

### 2.4 RAG Agent æ¶æ„è®¾è®¡

RAG Agent æ˜¯å°†æ£€ç´¢å’Œç”Ÿæˆç»“åˆåœ¨ä¸€èµ·çš„æ™ºèƒ½ä½“ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¿˜èƒ½åœ¨å›ç­”ä¸­å¼•ç”¨æ¥æºæ–‡æ¡£ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ çš„ RAG Agent åŸºäº LangChain çš„ `create_agent` API æ„å»ºï¼Œå°†æ£€ç´¢å™¨å°è£…ä¸ºä¸€ä¸ªå·¥å…·ä¾› Agent è°ƒç”¨ã€‚è¿™ç§æ¶æ„çš„ä¼˜åŠ¿åœ¨äºï¼šAgent èƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„å¤æ‚ç¨‹åº¦å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢ã€éœ€è¦æ£€ç´¢å¤šå°‘æ¬¡ï¼Œä½¿å¾—ç³»ç»Ÿæ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆã€‚

RAG Agent çš„æ ¸å¿ƒå·¥ä½œæµç¨‹å¦‚ä¸‹ï¼šé¦–å…ˆï¼ŒAgent ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡ŒçŸ¥è¯†æ£€ç´¢ï¼›å¦‚æœéœ€è¦ï¼ŒAgent è°ƒç”¨æ£€ç´¢å™¨å·¥å…·æœç´¢ç›¸å…³çš„æ–‡æ¡£å—ï¼›æ£€ç´¢å®Œæˆåï¼ŒAgent å°†ç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢ç»“æœç»„åˆæˆå®Œæ•´çš„æç¤ºè¯ï¼›æœ€åï¼ŒAgent è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œå¹¶åœ¨å›ç­”ä¸­æ ‡æ³¨æ¥æºæ–‡æ¡£ã€‚è¿™ç§æµç¨‹ä¸äººç±»ä¸“å®¶å›ç­”é—®é¢˜çš„è¿‡ç¨‹éå¸¸ç›¸ä¼¼ï¼šå…ˆæŸ¥æ‰¾èµ„æ–™ï¼Œå†åŸºäºèµ„æ–™ç»„ç»‡ç­”æ¡ˆã€‚

RAG Agent çš„ç³»ç»Ÿæç¤ºè¯è®¾è®¡æ˜¯å½±å“å…¶è¡¨ç°çš„å…³é”®å› ç´ ã€‚ä¸€ä¸ªå¥½çš„ç³»ç»Ÿæç¤ºè¯åº”è¯¥æ˜ç¡® Agent çš„è§’è‰²å®šä½ï¼ˆçŸ¥è¯†åº“é—®ç­”åŠ©æ‰‹ï¼‰ã€ä»»åŠ¡è¦æ±‚ï¼ˆå‡†ç¡®å›ç­”ã€å¼•ç”¨æ¥æºï¼‰ã€å›ç­”è§„èŒƒï¼ˆæ ¼å¼è¦æ±‚ã€é™åˆ¶æ¡ä»¶ï¼‰ç­‰ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ çš„é»˜è®¤ RAG ç³»ç»Ÿæç¤ºè¯å¼ºè°ƒä¸‰ä¸ªåŸåˆ™ï¼šå‡†ç¡®æ€§ï¼ˆä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹ï¼‰ã€å®Œæ•´æ€§ï¼ˆæä¾›è¯¦ç»†å›ç­”ï¼‰ã€æ¸…æ™°æ€§ï¼ˆä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€ï¼‰ã€‚åŒæ—¶è¦æ±‚ Agent åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå‚è€ƒçš„æ–‡æ¡£æ¥æºã€‚

å¯¹è¯å¼ RAG æ˜¯å¯¹åŸºç¡€ RAG çš„å¢å¼ºï¼Œæ”¯æŒå¤šè½®å¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œç”¨æˆ·çš„å½“å‰é—®é¢˜å¯èƒ½å¼•ç”¨äº†ä¹‹å‰å¯¹è¯ä¸­æåˆ°çš„æ¦‚å¿µæˆ–å®ä½“ï¼Œéœ€è¦ç»“åˆå¯¹è¯å†å²æ‰èƒ½å‡†ç¡®ç†è§£ã€‚QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ æä¾›äº†ä¸“é—¨çš„å¯¹è¯å¼ RAG Agent å®ç°ï¼Œå®ƒåœ¨ç³»ç»Ÿæç¤ºè¯ä¸­å¢åŠ äº†å¯¹å¯¹è¯å†å²çš„è¯´æ˜ï¼Œå¹¶è¦æ±‚ Agent åœ¨å›ç­”æ—¶è€ƒè™‘ä¸Šä¸‹æ–‡å…³ç³»ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ç”¨æˆ·å¯ä»¥è‡ªç„¶åœ°è¿›è¡Œè¿½é—®å’Œæ¾„æ¸…ã€‚

## å®æ“æ­¥éª¤

### 2.1 RAG å¼€å‘ç¯å¢ƒå‡†å¤‡

RAG ç³»ç»Ÿçš„å¼€å‘éœ€è¦ä¸€äº›é¢å¤–çš„ä¾èµ–åŒ…ã€‚é¦–å…ˆç¡®ä¿å·²ç»å®Œæˆäº†ç¬¬ä¸€ç« çš„ç¯å¢ƒé…ç½®ï¼Œç„¶åå®‰è£… RAG ç›¸å…³çš„ä¾èµ–ï¼š

```bash
# å®‰è£… RAG ç›¸å…³ä¾èµ–
cd backend
pip install faiss-cpu pypdf markdown beautifulsoup4 lxml
```

å¯¹äºéœ€è¦å¤„ç†ç‰¹æ®Šæ–‡æ¡£æ ¼å¼çš„åœºæ™¯ï¼Œå¯èƒ½è¿˜éœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–ï¼š

```bash
# å¯é€‰ä¾èµ–
pip install pandas openpyxl   # Excel æ–‡ä»¶å¤„ç†
pip install python-docx       # Word æ–‡æ¡£å¤„ç†
pip install pytesseract       # å›¾ç‰‡ OCR å¤„ç†
```

åˆ›å»º RAG ç³»ç»Ÿçš„ç›®å½•ç»“æ„ï¼š

```bash
mkdir -p data/documents       # åŸå§‹æ–‡æ¡£ç›®å½•
mkdir -p data/indexes         # å‘é‡ç´¢å¼•ç›®å½•
mkdir -p data/processed       # å¤„ç†åçš„æ–‡æ¡£ç›®å½•
```

å‡†å¤‡æµ‹è¯•æ–‡æ¡£ï¼Œå¯ä»¥æ”¾å…¥ä¸€äº› Markdown æˆ–æ–‡æœ¬æ–‡ä»¶ï¼š

```bash
# ç¤ºä¾‹ï¼šåˆ›å»ºæµ‹è¯•æ–‡æ¡£
echo "# æµ‹è¯•æ–‡æ¡£\n\nè¿™æ˜¯ä¸€ä»½å…³äºäººå·¥æ™ºèƒ½çš„ä»‹ç»ã€‚\n\n## æœºå™¨å­¦ä¹ \n\næœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚" > data/documents/intro.txt
```

### 2.2 æ–‡æ¡£åŠ è½½å™¨å®ç°

é¦–å…ˆæŸ¥çœ‹ QSTæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ ä¸­çš„æ–‡æ¡£åŠ è½½å™¨å®ç°ï¼š

```python
# rag/loaders.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

import os
from pathlib import Path
from typing import List, Union, Dict, Any
from langchain_core.documents import Document


class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨å·¥å‚ç±»"""
    
    @staticmethod
    def load(file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£å¹¶è½¬æ¢ä¸º Document å¯¹è±¡åˆ—è¡¨
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            Document å¯¹è±¡åˆ—è¡¨
        """
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            return DocumentLoader._load_pdf(file_path)
        elif file_ext == '.md':
            return DocumentLoader._load_markdown(file_path)
        elif file_ext in ['.txt', '.text']:
            return DocumentLoader._load_text(file_path)
        elif file_ext == '.html':
            return DocumentLoader._load_html(file_path)
        elif file_ext == '.json':
            return DocumentLoader._load_json(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_ext}")
    
    @staticmethod
    def _load_pdf(file_path: str) -> List[Document]:
        """åŠ è½½ PDF æ–‡æ¡£"""
        from pypdf import PdfReader
        
        reader = PdfReader(file_path)
        documents = []
        
        for page_num, page in enumerate(reader.pages):
            text = page.extract_text()
            if text.strip():
                doc = Document(
                    page_content=text,
                    metadata={
                        "source": file_path,
                        "page": page_num,
                        "total_pages": len(reader.pages)
                    }
                )
                documents.append(doc)
        
        return documents
    
    @staticmethod
    def _load_markdown(file_path: str) -> List[Document]:
        """åŠ è½½ Markdown æ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç®€å•å¤„ç†ï¼šæ•´ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ä¸ª Document
        doc = Document(
            page_content=content,
            metadata={
                "source": file_path,
                "type": "markdown"
            }
        )
        
        return [doc]
    
    @staticmethod
    def _load_text(file_path: str) -> List[Document]:
        """åŠ è½½çº¯æ–‡æœ¬æ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        doc = Document(
            page_content=content,
            metadata={
                "source": file_path,
                "type": "text"
            }
        )
        
        return [doc]
    
    @staticmethod
    def _load_html(file_path: str) -> List[Document]:
        """åŠ è½½ HTML æ–‡æ¡£"""
        from bs4 import BeautifulSoup
        
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'lxml')
        
        # æå–æ­£æ–‡å†…å®¹ï¼ˆå»é™¤ scriptã€style ç­‰æ ‡ç­¾ï¼‰
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        text = soup.get_text(separator='\n', strip=True)
        
        doc = Document(
            page_content=text,
            metadata={
                "source": file_path,
                "type": "html"
            }
        )
        
        return [doc]
    
    @staticmethod
    def _load_json(file_path: str) -> List[Document]:
        """åŠ è½½ JSON æ–‡æ¡£"""
        import json
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = []
        
        # é€’å½’æå– JSON ä¸­çš„æ–‡æœ¬
        def extract_text(obj, path=""):
            if isinstance(obj, str):
                return [Document(
                    page_content=obj,
                    metadata={"source": file_path, "path": path}
                )]
            elif isinstance(obj, dict):
                result = []
                for key, value in obj.items():
                    result.extend(extract_text(value, f"{path}.{key}"))
                return result
            elif isinstance(obj, list):
                result = []
                for i, item in enumerate(obj):
                    result.extend(extract_text(item, f"{path}[{i}]"))
                return result
            return []
        
        return extract_text(data)


def load_document(file_path: str) -> List[Document]:
    """ä¾¿æ·çš„æ–‡æ¡£åŠ è½½å‡½æ•°"""
    return DocumentLoader.load(file_path)
```

ç°åœ¨åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š

```python
# examples/ch02/01_document_loading.py
"""
æ–‡æ¡£åŠ è½½ç¤ºä¾‹
å­¦ä¹ å¦‚ä½•åŠ è½½ä¸åŒæ ¼å¼çš„æ–‡æ¡£
"""

import os
from rag.loaders import load_document
from config import get_logger

logger = get_logger(__name__)


def main():
    """æ¼”ç¤ºå„ç§æ–‡æ¡£æ ¼å¼çš„åŠ è½½"""
    
    # æµ‹è¯•æ–‡æ¡£ç›®å½•
    docs_dir = "./data/documents"
    
    if not os.path.exists(docs_dir):
        os.makedirs(docs_dir)
        logger.info(f"åˆ›å»ºæµ‹è¯•æ–‡æ¡£ç›®å½•: {docs_dir}")
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        create_test_documents()
    
    # åŠ è½½å¹¶æ˜¾ç¤ºæ–‡æ¡£ä¿¡æ¯
    for filename in os.listdir(docs_dir):
        filepath = os.path.join(docs_dir, filename)
        if os.path.isfile(filepath):
            try:
                docs = load_document(filepath)
                logger.info(f"\nğŸ“„ æ–‡ä»¶: {filename}")
                logger.info(f"   åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£å—")
                
                # æ˜¾ç¤ºæ¯ä¸ªå—çš„ä¿¡æ¯
                for i, doc in enumerate(docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    content_preview = doc.page_content[:100].replace('\n', ' ')
                    logger.info(f"   å— {i+1}: {content_preview}...")
                    
                    # æ˜¾ç¤ºå…ƒæ•°æ®
                    if doc.metadata:
                        logger.info(f"         å…ƒæ•°æ®: {doc.metadata}")
                        
            except Exception as e:
                logger.error(f"âŒ åŠ è½½å¤±è´¥ {filename}: {e}")


def create_test_documents():
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£"""
    
    # åˆ›å»º Markdown æµ‹è¯•æ–‡æ¡£
    md_content = """# äººå·¥æ™ºèƒ½ç®€ä»‹

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§° AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

## æœºå™¨å­¦ä¹ 

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ ¸å¿ƒå­é¢†åŸŸï¼Œå®ƒä¸“é—¨ç ”ç©¶è®¡ç®—æœºæ€æ ·æ¨¡æ‹Ÿæˆ–å®ç°äººç±»çš„å­¦ä¹ è¡Œä¸ºï¼Œ
ä»¥è·å–æ–°çš„çŸ¥è¯†æˆ–æŠ€èƒ½ï¼Œé‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æ„ï¼Œä½¿ä¹‹ä¸æ–­æ”¹å–„è‡ªèº«çš„æ€§èƒ½ã€‚

## æ·±åº¦å­¦ä¹ 

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚
æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

## åº”ç”¨é¢†åŸŸ

- **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€äººè„¸è¯†åˆ«
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€æ™ºèƒ½é—®ç­”
- **è¯­éŸ³è¯†åˆ«**ï¼šè¯­éŸ³è½¬æ–‡å­—ã€æ–‡å­—è½¬è¯­éŸ³
"""
    
    with open("./data/documents/ai_intro.md", 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    # åˆ›å»ºæ–‡æœ¬æµ‹è¯•æ–‡æ¡£
    txt_content = """è¿™æ˜¯ä¸€ä»½æµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯æ–‡æœ¬åŠ è½½åŠŸèƒ½ã€‚

æ–‡æ¡£ä¸­å¯ä»¥åŒ…å«å¤šè¡Œæ–‡æœ¬ï¼Œ
ä»¥åŠå„ç§æ ¼å¼çš„å†…å®¹ã€‚

ç¬¬äºŒæ®µè½çš„å†…å®¹ã€‚
"""
    
    with open("./data/documents/test.txt", 'w', encoding='utf-8') as f:
        f.write(txt_content)
    
    logger.info("âœ… æµ‹è¯•æ–‡æ¡£åˆ›å»ºå®Œæˆ")


if __name__ == "__main__":
    main()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python examples/ch02/01_document_loading.py
```

### 2.3 æ–‡æœ¬åˆ†å—ç­–ç•¥å®ç°

æ¥ä¸‹æ¥å®ç°æ–‡æœ¬åˆ†å—åŠŸèƒ½ï¼š

```python
# rag/splitters.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

from typing import List, Optional, Callable
from langchain_core.documents import Document
from langchain_experimental.text_splitter import (
    RecursiveCharacterTextSplitter as BaseSplitter
)


class TextSplitter:
    """æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 100,
        separators: Optional[List[str]] = None,
        length_function: Callable[[str], int] = len,
        keep_separator: bool = False
    ):
        """
        åˆå§‹åŒ–åˆ†å—å™¨
        
        Args:
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap: ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
            separators: åˆ†éš”ç¬¦åˆ—è¡¨
            length_function: è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°
            keep_separator: æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or [
            "\n\n",  # æ®µè½åˆ†éš”
            "\n",    # è¡Œåˆ†éš”
            "ã€‚",    # å¥å·
            "ï¼",
            "ï¼Ÿ",
            "ï¼›",    # åˆ†å·
            " ",     # å•è¯åˆ†éš”
            ""       # å­—ç¬¦åˆ†éš”
        ]
        
        # ä½¿ç”¨ LangChain çš„é€’å½’å­—ç¬¦åˆ†å—å™¨
        self._splitter = BaseSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=self.separators,
            length_function=length_function,
            keep_separator=keep_separator
        )
    
    def split_documents(
        self,
        documents: List[Document],
        add_start_index: bool = True
    ) -> List[Document]:
        """
        åˆ†å‰²æ–‡æ¡£åˆ—è¡¨
        
        Args:
            documents: Document å¯¹è±¡åˆ—è¡¨
            add_start_index: æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•å…ƒæ•°æ®
            
        Returns:
            åˆ†å‰²åçš„ Document åˆ—è¡¨
        """
        chunks = self._splitter.split_documents(documents)
        
        # æ·»åŠ èµ·å§‹ç´¢å¼•
        if add_start_index:
            for chunk in chunks:
                if "start_index" not in chunk.metadata:
                    chunk.metadata["start_index"] = 0
        
        logger.info(f"ğŸ“š åˆ†å—å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªå—")
        
        return chunks
    
    def split_text(
        self,
        text: str,
        add_start_index: bool = True
    ) -> List[str]:
        """
        åˆ†å‰²å•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            add_start_index: æ˜¯å¦æ·»åŠ èµ·å§‹ç´¢å¼•
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        chunks = self._splitter.split_text(text)
        
        logger.info(f"âœ‚ï¸  æ–‡æœ¬åˆ†å—: {len(text)} å­—ç¬¦ -> {len(chunks)} ä¸ªå—")
        
        return chunks


def recursive_split(
    documents: List[Document],
    chunk_size: int = 1000,
    chunk_overlap: int = 100
) -> List[Document]:
    """
    ä¾¿æ·çš„é€’å½’åˆ†å—å‡½æ•°
    
    Args:
        documents: Document å¯¹è±¡åˆ—è¡¨
        chunk_size: å—å¤§å°
        chunk_overlap: é‡å å¤§å°
        
    Returns:
        åˆ†å‰²åçš„æ–‡æ¡£åˆ—è¡¨
    """
    splitter = TextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    return splitter.split_documents(documents)
```

åˆ›å»ºåˆ†å—æµ‹è¯•è„šæœ¬ï¼š

```python
# examples/ch02/02_text_splitter.py
"""
æ–‡æœ¬åˆ†å—ç¤ºä¾‹
å­¦ä¹ ä¸åŒçš„åˆ†å—ç­–ç•¥
"""

import os
from rag.loaders import load_document
from rag.splitters import recursive_split, TextSplitter
from config import get_logger

logger = get_logger(__name__)


def main():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†å—åŠŸèƒ½"""
    
    # é¦–å…ˆåŠ è½½æ–‡æ¡£
    doc_path = "./data/documents/ai_intro.md"
    if not os.path.exists(doc_path):
        logger.error("è¯·å…ˆè¿è¡Œ 01_document_loading.py åˆ›å»ºæµ‹è¯•æ–‡æ¡£")
        return
    
    documents = load_document(doc_path)
    logger.info(f"ğŸ“„ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•ä¸åŒçš„åˆ†å—å‚æ•°
    test_cases = [
        {"chunk_size": 500, "chunk_overlap": 50},
        {"chunk_size": 1000, "chunk_overlap": 100},
        {"chunk_size": 200, "chunk_overlap": 20},
    ]
    
    for params in test_cases:
        print("\n" + "=" * 60)
        logger.info(f"ğŸ”§ åˆ†å—å‚æ•°: size={params['chunk_size']}, overlap={params['chunk_overlap']}")
        print("=" * 60)
        
        chunks = recursive_split(
            documents,
            chunk_size=params['chunk_size'],
            chunk_overlap=params['chunk_overlap']
        )
        
        for i, chunk in enumerate(chunks):
            content_preview = chunk.page_content[:80].replace('\n', ' ')
            logger.info(f"   å— {i+1} ({len(chunk.page_content)} å­—ç¬¦): {content_preview}...")


def advanced_splitter_demo():
    """é«˜çº§åˆ†å—å™¨æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ”§ é«˜çº§åˆ†å—ç­–ç•¥æ¼”ç¤º")
    print("=" * 60)
    
    # è‡ªå®šä¹‰åˆ†éš”ç¬¦çš„åˆ†å—å™¨
    custom_splitter = TextSplitter(
        chunk_size=300,
        chunk_overlap=30,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )
    
    test_text = """
    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„é‡è¦åˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼
    æ·±åº¦å­¦ä¹ åœ¨è¿‘å¹´å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ä¹Ÿæ˜¯AIçš„é‡è¦åº”ç”¨é¢†åŸŸã€‚
    è®¡ç®—æœºè§†è§‰èƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ï¼›è¯­éŸ³è¯†åˆ«è®©æœºå™¨å¬æ‡‚äººç±»è¯­è¨€ã€‚
    """
    
    logger.info("ğŸ“ æµ‹è¯•æ–‡æœ¬:")
    logger.info(f"   {test_text.strip()}")
    
    chunks = custom_splitter.split_text(test_text)
    
    for i, chunk in enumerate(chunks):
        logger.info(f"   å— {i+1}: {chunk}")


if __name__ == "__main__":
    main()
    advanced_splitter_demo()
```

### 2.4 å‘é‡ç´¢å¼•æ„å»ºä¸å­˜å‚¨

ç°åœ¨å­¦ä¹ å¦‚ä½•æ„å»ºå’Œå­˜å‚¨å‘é‡ç´¢å¼•ï¼š

```python
# rag/embeddings.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

from typing import Optional, List
from langchain_openai import OpenAIEmbeddings
from config import settings, get_logger

logger = get_logger(__name__)


def get_embeddings(
    model: str = "text-embedding-3-small",
    dimensions: Optional[int] = None,
    **kwargs
) -> OpenAIEmbeddings:
    """
    è·å– Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: Embedding æ¨¡å‹åç§°
        dimensions: è¾“å‡ºç»´åº¦ï¼ˆå¯é€‰ï¼ŒæŸäº›æ¨¡å‹æ”¯æŒé™ç»´ï¼‰
        **kwargs: å…¶ä»–ä¼ é€’ç»™ OpenAIEmbeddings çš„å‚æ•°
        
    Returns:
        OpenAIEmbeddings å®ä¾‹
    """
    embedding_config = {
        "model": model,
        "api_key": settings.openai_api_key,
        "base_url": settings.openai_api_base,
    }
    
    if dimensions:
        embedding_config["dimensions"] = dimensions
    
    embedding_config.update(kwargs)
    
    logger.info(f"ğŸ”¢ åˆå§‹åŒ– Embedding æ¨¡å‹: {model}")
    
    return OpenAIEmbeddings(**embedding_config)


# rag/vector_stores.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

import os
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_core.vectorstores import VectorStore
from rag.embeddings import get_embeddings
from config import get_logger

logger = get_logger(__name__)


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self,
        embeddings: OpenAIEmbeddings = None,
        persist_directory: str = "./data/indexes"
    ):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            embeddings: Embedding æ¨¡å‹å®ä¾‹
            persist_directory: ç´¢å¼•ä¿å­˜ç›®å½•
        """
        self.embeddings = embeddings or get_embeddings()
        self.persist_directory = persist_directory
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(persist_directory, exist_ok=True)
    
    def create_from_documents(
        self,
        documents: List[Document],
        index_name: str = "default"
    ) -> FAISS:
        """
        ä»æ–‡æ¡£åˆ›å»ºå‘é‡ç´¢å¼•
        
        Args:
            documents: Document å¯¹è±¡åˆ—è¡¨
            index_name: ç´¢å¼•åç§°
            
        Returns:
            FAISS å‘é‡å­˜å‚¨å®ä¾‹
        """
        logger.info(f"ğŸ’¾ åˆ›å»ºå‘é‡ç´¢å¼•: {index_name}")
        logger.info(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")
        
        vector_store = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        
        # ä¿å­˜ç´¢å¼•
        self._save(vector_store, index_name)
        
        logger.info(f"âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
        
        return vector_store
    
    def save(self, vector_store: FAISS, index_name: str = "default"):
        """ä¿å­˜å‘é‡ç´¢å¼•åˆ°ç£ç›˜"""
        self._save(vector_store, index_name)
    
    def _save(self, vector_store: FAISS, index_name: str):
        """å†…éƒ¨ä¿å­˜æ–¹æ³•"""
        save_path = os.path.join(self.persist_directory, index_name)
        vector_store.save_local(save_path)
        logger.info(f"   ç´¢å¼•å·²ä¿å­˜: {save_path}")
    
    def load(
        self,
        index_name: str = "default"
    ) -> Optional[FAISS]:
        """
        ä»ç£ç›˜åŠ è½½å‘é‡ç´¢å¼•
        
        Args:
            index_name: ç´¢å¼•åç§°
            
        Returns:
            FAISS å‘é‡å­˜å‚¨å®ä¾‹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        load_path = os.path.join(self.persist_directory, index_name)
        
        if not os.path.exists(load_path):
            logger.warning(f"âš ï¸  ç´¢å¼•ä¸å­˜åœ¨: {load_path}")
            return None
        
        logger.info(f"ğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•: {index_name}")
        
        vector_store = FAISS.load_local(
            load_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        
        logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ")
        
        return vector_store
    
    def list_indexes(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç´¢å¼•"""
        if not os.path.exists(self.persist_directory):
            return []
        
        indexes = [
            name for name in os.listdir(self.persist_directory)
            if os.path.isdir(os.path.join(self.persist_directory, name))
        ]
        
        return indexes


def create_vector_store(
    documents: List[Document] = None,
    embedding = None,
    persist_directory: str = "./data/indexes"
) -> FAISS:
    """
    åˆ›å»ºå‘é‡å­˜å‚¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        documents: Document å¯¹è±¡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        embedding: Embedding æ¨¡å‹
        persist_directory: ä¿å­˜ç›®å½•
        
    Returns:
        FAISS å‘é‡å­˜å‚¨å®ä¾‹
    """
    manager = VectorStoreManager(
        embeddings=embedding,
        persist_directory=persist_directory
    )
    
    if documents:
        return manager.create_from_documents(documents)
    else:
        return FAISS(embedding=embedding)


def load_vector_store(
    index_path: str,
    embedding = None
) -> FAISS:
    """
    åŠ è½½å‘é‡å­˜å‚¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        index_path: ç´¢å¼•è·¯å¾„æˆ–åç§°
        embedding: Embedding æ¨¡å‹
        
    Returns:
        FAISS å‘é‡å­˜å‚¨å®ä¾‹
    """
    manager = VectorStoreManager(persist_directory=index_path)
    return manager.load(index_path)
```

åˆ›å»ºå‘é‡ç´¢å¼•æ„å»ºè„šæœ¬ï¼š

```python
# examples/ch02/03_build_vector_index.py
"""
å‘é‡ç´¢å¼•æ„å»ºç¤ºä¾‹
å­¦ä¹ å¦‚ä½•åˆ›å»ºå’ŒæŒä¹…åŒ–å‘é‡ç´¢å¼•
"""

import os
from rag.loaders import load_document
from rag.splitters import recursive_split
from rag.embeddings import get_embeddings
from rag.vector_stores import create_vector_store, VectorStoreManager
from config import get_logger

logger = get_logger(__name__)


def main():
    """æ„å»ºå‘é‡ç´¢å¼•çš„å®Œæ•´æµç¨‹"""
    
    print("=" * 60)
    print("ğŸ“š å‘é‡ç´¢å¼•æ„å»ºå®æˆ˜")
    print("=" * 60)
    
    # 1. å‡†å¤‡æ–‡æ¡£
    docs_dir = "./data/documents"
    if not os.path.exists(docs_dir):
        logger.error("è¯·å…ˆè¿è¡Œ 01_document_loading.py åˆ›å»ºæµ‹è¯•æ–‡æ¡£")
        return
    
    print("\nğŸ“„ æ­¥éª¤ 1: åŠ è½½æ–‡æ¡£")
    all_documents = []
    
    for filename in os.listdir(docs_dir):
        filepath = os.path.join(docs_dir, filename)
        if os.path.isfile(filepath):
            try:
                docs = load_document(filepath)
                all_documents.extend(docs)
                logger.info(f"   åŠ è½½ {filename}: {len(docs)} ä¸ªå—")
            except Exception as e:
                logger.error(f"   åŠ è½½å¤±è´¥ {filename}: {e}")
    
    print(f"   å…±åŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£å—")
    
    # 2. åˆ†å—å¤„ç†
    print("\nâœ‚ï¸  æ­¥éª¤ 2: æ–‡æœ¬åˆ†å—")
    chunks = recursive_split(
        all_documents,
        chunk_size=500,
        chunk_overlap=50
    )
    print(f"   åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
    
    # 3. ç”Ÿæˆ Embedding
    print("\nğŸ”¢ æ­¥éª¤ 3: ç”Ÿæˆå‘é‡")
    print("   åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embeddings = get_embeddings()
    
    # 4. åˆ›å»ºå‘é‡ç´¢å¼•
    print("\nğŸ’¾ æ­¥éª¤ 4: åˆ›å»ºå‘é‡ç´¢å¼•")
    index_dir = "./data/indexes/sample"
    
    vector_store = create_vector_store(
        documents=chunks,
        embedding=embeddings,
        persist_directory=index_dir
    )
    
    print(f"\nâœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼")
    print(f"   ç´¢å¼•ä½ç½®: {index_dir}")
    print(f"   æ–‡æ¡£å—æ•°: {len(chunks)}")


def manage_indexes():
    """ç´¢å¼•ç®¡ç†æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ”§ ç´¢å¼•ç®¡ç†åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    manager = VectorStoreManager(persist_directory="./data/indexes")
    
    # åˆ—å‡ºæ‰€æœ‰ç´¢å¼•
    print("\nğŸ“‹ å¯ç”¨çš„ç´¢å¼•:")
    indexes = manager.list_indexes()
    if indexes:
        for idx in indexes:
            print(f"   - {idx}")
    else:
        print("   æ²¡æœ‰æ‰¾åˆ°ç´¢å¼•")
    
    # åŠ è½½æŒ‡å®šç´¢å¼•
    print("\nğŸ“‚ åŠ è½½ç´¢å¼•:")
    vs = manager.load("sample")
    if vs:
        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        print(f"   ç´¢å¼•æ–‡æ¡£æ•°: {len(vs.docstore._dict)}")


if __name__ == "__main__":
    main()
    manage_indexes()
```

### 2.5 æ£€ç´¢å™¨é…ç½®ä¸ RAG Agent åˆ›å»º

æœ€åï¼Œåˆ›å»ºæ£€ç´¢å™¨å¹¶æ„å»ºå®Œæ•´çš„ RAG Agentï¼š

```python
# rag/retrievers.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

from typing import Dict, List, Any, Optional
from langchain_core.retrievers import BaseRetriever
from langchain_core.vectorstores import VectorStore
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.documents import Document
from config import get_logger

logger = get_logger(__name__)


class VectorStoreRetriever:
    """å‘é‡å­˜å‚¨æ£€ç´¢å™¨å°è£…"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        search_type: str = "similarity",
        search_kwargs: Optional[Dict] = None
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹
            search_type: æ£€ç´¢ç±»å‹
            search_kwargs: æ£€ç´¢å‚æ•°
        """
        self.vector_store = vector_store
        self.search_type = search_type
        self.search_kwargs = search_kwargs or {}
        
        # åˆ›å»ºåº•å±‚çš„ LangChain æ£€ç´¢å™¨
        self._retriever = vector_store.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
    
    def invoke(self, query: str) -> List[Document]:
        """åŒæ­¥æ£€ç´¢"""
        return self._retriever.invoke(query)
    
    async def ainvoke(self, query: str) -> List[Document]:
        """å¼‚æ­¥æ£€ç´¢"""
        return await self._retriever.ainvoke(query)
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        return self.invoke(query)
    
    def config_info(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨é…ç½®ä¿¡æ¯"""
        return {
            "search_type": self.search_type,
            "search_kwargs": self.search_kwargs,
            "docstore_count": len(self.vector_store.docstore._dict)
        }


def create_retriever(
    vector_store: VectorStore,
    search_type: str = "similarity",
    k: int = 5,
    **kwargs
) -> VectorStoreRetriever:
    """
    åˆ›å»ºæ£€ç´¢å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
        search_type: æ£€ç´¢ç±»å‹ï¼ˆsimilarity/mmr/similarity_score_thresholdï¼‰
        k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        **kwargs: å…¶ä»–æ£€ç´¢å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„æ£€ç´¢å™¨å®ä¾‹
    """
    # æ ¹æ®æ£€ç´¢ç±»å‹è®¾ç½®å‚æ•°
    search_kwargs = {"k": k}
    
    if search_type == "similarity_score_threshold":
        search_kwargs["score_threshold"] = kwargs.pop("score_threshold", 0.7)
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    search_kwargs.update(kwargs)
    
    logger.info(f"ğŸ” åˆ›å»ºæ£€ç´¢å™¨: type={search_type}, k={k}")
    
    return VectorStoreRetriever(
        vector_store=vector_store,
        search_type=search_type,
        search_kwargs=search_kwargs
    )


# rag/rag_agent.pyï¼ˆæ ¸å¿ƒä»£ç è§£æï¼‰

from langchain.agents import create_agent
from rag.retrievers import create_retriever
from config import get_logger

logger = get_logger(__name__)


# RAG Agent é»˜è®¤ç³»ç»Ÿæç¤ºè¯
DEFAULT_RAG_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”åŸºäºçŸ¥è¯†åº“çš„é—®é¢˜ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ä½¿ç”¨ knowledge_base å·¥å…·æœç´¢ç›¸å…³ä¿¡æ¯
2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜
3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®åœ°å‘Šè¯‰ç”¨æˆ·
4. åœ¨å›ç­”ä¸­å¼•ç”¨æ¥æºæ–‡æ¡£ï¼ˆå¦‚æœæœ‰ source ä¿¡æ¯ï¼‰

å›ç­”è¦æ±‚ï¼š
- å‡†ç¡®ï¼šä¸¥æ ¼åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
- å®Œæ•´ï¼šå°½å¯èƒ½æä¾›è¯¦ç»†çš„å›ç­”
- æ¸…æ™°ï¼šä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€
- å¼•ç”¨ï¼šåœ¨å›ç­”æœ«å°¾åˆ—å‡ºå‚è€ƒçš„æ–‡æ¡£æ¥æº
"""


def create_rag_agent(
    retriever,
    model: str = None,
    system_prompt: str = None,
    tool_name: str = "knowledge_base",
    tool_description: str = None,
    **kwargs
):
    """
    åˆ›å»º RAG Agent
    
    Args:
        retriever: æ£€ç´¢å™¨å®ä¾‹
        model: æ¨¡å‹æ ‡è¯†ç¬¦
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        tool_name: æ£€ç´¢å·¥å…·åç§°
        tool_description: æ£€ç´¢å·¥å…·æè¿°
        **kwargs: å…¶ä»– create_agent å‚æ•°
        
    Returns:
        Agent å®ä¾‹
    """
    from core.models import get_model_string
    
    logger.info("ğŸ¤– åˆ›å»º RAG Agent")
    
    # ä½¿ç”¨é»˜è®¤æ¨¡å‹
    if model is None:
        model = get_model_string()
    
    # ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤ºè¯
    if system_prompt is None:
        system_prompt = DEFAULT_RAG_SYSTEM_PROMPT
    
    # åˆ›å»ºæ£€ç´¢å™¨å·¥å…·
    if tool_description is None:
        tool_description = (
            "æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚"
            "å½“éœ€è¦å›ç­”å…³äºæ–‡æ¡£å†…å®¹çš„é—®é¢˜æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚"
            "è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚"
        )
    
    retriever_tool = create_retriever_tool(
        retriever=retriever,
        name=tool_name,
        description=tool_description,
    )
    
    tools = [retriever_tool]
    
    # åˆ›å»º Agent
    agent = create_agent(
        model=model,
        tools=tools,
        system_prompt=system_prompt,
        **kwargs,
    )
    
    logger.info(f"âœ… RAG Agent åˆ›å»ºæˆåŠŸ")
    
    return agent


def create_retriever_tool(
    retriever,
    name: str = "knowledge_base",
    description: str = None
):
    """åˆ›å»ºæ£€ç´¢å™¨å·¥å…·"""
    from langchain_core.tools import create_retriever_tool
    
    if description is None:
        description = "ä»çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ã€‚"
    
    tool = create_retriever_tool(
        retriever=retriever,
        name=name,
        description=description
    )
    
    return tool
```

åˆ›å»º RAG Agent æµ‹è¯•è„šæœ¬ï¼š

```python
# examples/ch02/04_rag_agent.py
"""
RAG Agent ç¤ºä¾‹
å­¦ä¹ å¦‚ä½•æ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
"""

import os
from rag.loaders import load_document
from rag.splitters import recursive_split
from rag.embeddings import get_embeddings
from rag.vector_stores import load_vector_store
from rag.retrievers import create_retriever, create_retriever_tool
from rag.rag_agent import create_rag_agent
from config import get_logger

logger = get_logger(__name__)


def main():
    """è¿è¡Œå®Œæ•´çš„ RAG é—®ç­”ç¤ºä¾‹"""
    
    print("=" * 60)
    print("ğŸ¯ RAG çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    print("=" * 60)
    
    # 1. åŠ è½½å‘é‡ç´¢å¼•
    index_dir = "./data/indexes/sample"
    
    if not os.path.exists(index_dir):
        logger.error("è¯·å…ˆè¿è¡Œ 03_build_vector_index.py åˆ›å»ºç´¢å¼•")
        return
    
    print("\nğŸ“‚ åŠ è½½å‘é‡ç´¢å¼•...")
    embeddings = get_embeddings()
    vector_store = load_vector_store(index_dir, embeddings)
    
    if vector_store is None:
        logger.error("ç´¢å¼•åŠ è½½å¤±è´¥")
        return
    
    doc_count = len(vector_store.docstore._dict)
    print(f"   ç´¢å¼•åŒ…å« {doc_count} ä¸ªæ–‡æ¡£å—")
    
    # 2. åˆ›å»ºæ£€ç´¢å™¨
    print("\nğŸ” åˆ›å»ºæ£€ç´¢å™¨...")
    retriever = create_retriever(
        vector_store=vector_store,
        search_type="similarity",
        k=3  # è¿”å›æœ€ç›¸ä¼¼çš„ 3 ä¸ªæ–‡æ¡£
    )
    
    # æ˜¾ç¤ºæ£€ç´¢å™¨é…ç½®
    info = retriever.config_info()
    print(f"   æ£€ç´¢ç±»å‹: {info['search_type']}")
    print(f"   è¿”å›æ•°é‡: {info['search_kwargs']['k']}")
    
    # 3. åˆ›å»º RAG Agent
    print("\nğŸ¤– åˆ›å»º RAG Agent...")
    agent = create_rag_agent(retriever=retriever)
    
    # 4. æµ‹è¯•é—®ç­”
    print("\n" + "=" * 60)
    print("ğŸ’¬ é—®ç­”æµ‹è¯•")
    print("=" * 60)
    
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "äººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨é¢†åŸŸï¼Ÿ",
        "è¿™ä¸ªé—®é¢˜æ–‡æ¡£ä¸­æ²¡æœ‰æ¶‰åŠ",
    ]
    
    for question in questions:
        print(f"\nğŸ“ é—®é¢˜: {question}")
        print("-" * 40)
        
        try:
            response = agent.invoke(question)
            print(f"ğŸ¤– å›ç­”:\n{response}")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


def test_retrieval():
    """ç›´æ¥æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ” æ£€ç´¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    index_dir = "./data/indexes/sample"
    embeddings = get_embeddings()
    vector_store = load_vector_store(index_dir, embeddings)
    
    if vector_store is None:
        return
    
    retriever = create_retriever(vector_store, k=3)
    
    test_queries = [
        "æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ",
        "è®¡ç®—æœºè§†è§‰",
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è¯¢: {query}")
        print("-" * 40)
        
        docs = retriever.invoke(query)
        
        for i, doc in enumerate(docs, 1):
            content_preview = doc.page_content[:100].replace('\n', ' ')
            print(f"   ç»“æœ {i} (ç›¸ä¼¼åº¦ç›¸å…³): {content_preview}...")
            
            if doc.metadata:
                print(f"         æ¥æº: {doc.metadata.get('source', 'unknown')}")


if __name__ == "__main__":
    main()
    test_retrieval()
```

## æ•™å­¦è¦ç‚¹

### 3.1 åˆ†å—ç­–ç•¥çš„é€‰æ‹©ä¸ä¼˜åŒ–

åˆ†å—ç­–ç•¥æ˜¯ RAG ç³»ç»Ÿä¸­æœ€é‡è¦çš„è¶…å‚æ•°ä¹‹ä¸€ã€‚å—å¤§å°çš„é€‰æ‹©éœ€è¦æƒè¡¡æ£€ç´¢ç²¾åº¦å’Œä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚è¾ƒå¤§çš„å—ï¼ˆå¦‚ 1000-2000 å­—ç¬¦ï¼‰åŒ…å«æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†å¯èƒ½å¼•å…¥æ›´å¤šå™ªå£°ï¼›è¾ƒå°çš„å—ï¼ˆå¦‚ 200-500 å­—ç¬¦ï¼‰æ£€ç´¢ç²¾åº¦æ›´é«˜ï¼Œä½†å¯èƒ½ä¸¢å¤±é‡è¦çš„ä¸Šä¸‹æ–‡ã€‚å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸å»ºè®®ä»ä¸­ç­‰å¤§å°ï¼ˆå¦‚ 500-800 å­—ç¬¦ï¼‰å¼€å§‹ï¼Œç„¶åæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´ã€‚

é‡å å¤§å°æ˜¯å¦ä¸€ä¸ªéœ€è¦ä»”ç»†è°ƒæ•´çš„å‚æ•°ã€‚è®¾ç½®é‡å å¯ä»¥ç¡®ä¿é‡è¦ä¿¡æ¯ä¸ä¼šå› ä¸ºåˆ†å—è¾¹ç•Œè€Œè¢«åˆ‡æ–­ã€‚ä½†è¿‡å¤§çš„é‡å ä¼šå¢åŠ æ£€ç´¢çš„å†—ä½™æ€§å’Œæˆæœ¬ã€‚ä¸€èˆ¬å»ºè®®é‡å å¤§å°è®¾ç½®ä¸ºå—å¤§å°çš„ 10% åˆ° 20%ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå—å¤§å°æ˜¯ 1000 å­—ç¬¦ï¼Œé‡å å¯ä»¥è®¾ç½®ä¸º 100-200 å­—ç¬¦ã€‚

å¯¹äºç‰¹æ®Šç±»å‹çš„æ–‡æ¡£ï¼Œå¯ä»¥é‡‡ç”¨æ›´ç²¾ç»†çš„åˆ†å—ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œå¯¹äºæŠ€æœ¯æ–‡æ¡£ï¼Œå¯ä»¥æŒ‰ç…§æ ‡é¢˜å±‚çº§è¿›è¡Œåˆ†å—ï¼Œæ¯ä¸ªå—å¯¹åº”ä¸€ä¸ªå°èŠ‚ï¼›å¯¹äºä»£ç æ–‡æ¡£ï¼Œå¯ä»¥ä¿æŒä»£ç å—çš„å®Œæ•´æ€§ï¼Œä¸åœ¨ä»£ç å†…éƒ¨è¿›è¡Œåˆ†å‰²ï¼›å¯¹äºå¯¹è¯è®°å½•ï¼Œå¯ä»¥æŒ‰ç…§è½®æ¬¡è¿›è¡Œåˆ†å—ï¼Œä¿æŒå¯¹è¯çš„è¿è´¯æ€§ã€‚

### 3.2 æ£€ç´¢æ•ˆæœè¯„ä¼°ä¸ä¼˜åŒ–

æ£€ç´¢æ•ˆæœçš„è¯„ä¼°é€šå¸¸ä½¿ç”¨å‘½ä¸­ç‡ï¼ˆHit Rateï¼‰å’Œå¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ç­‰æŒ‡æ ‡ã€‚å‘½ä¸­ç‡æŒ‡æ£€ç´¢ç»“æœä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹ï¼›MRR è¡¡é‡æ­£ç¡®ç­”æ¡ˆåœ¨æ£€ç´¢ç»“æœä¸­çš„å¹³å‡æ’åä½ç½®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥é€šè¿‡äººå·¥è¯„ä¼°æˆ–è‡ªåŠ¨è¯„ä¼°ï¼ˆå¦‚ä½¿ç”¨å¦ä¸€ä¸ªè¯­è¨€æ¨¡å‹åˆ¤æ–­æ£€ç´¢ç»“æœæ˜¯å¦ç›¸å…³ï¼‰æ¥é‡åŒ–æ£€ç´¢æ•ˆæœã€‚

æ£€ç´¢ä¼˜åŒ–çš„å¸¸è§ç­–ç•¥åŒ…æ‹¬ï¼šæŸ¥è¯¢æ”¹å†™ï¼ˆå°†ç”¨æˆ·é—®é¢˜æ”¹å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ï¼‰ã€æ··åˆæ£€ç´¢ï¼ˆç»“åˆå…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢ï¼‰ã€ç»“æœé‡æ’åºï¼ˆä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹å¯¹åˆæ­¥æ£€ç´¢ç»“æœè¿›è¡Œé‡æ–°æ’åºï¼‰ã€æŸ¥è¯¢æ‰©å±•ï¼ˆå°†ä¸€ä¸ªæŸ¥è¯¢æ‰©å±•ä¸ºå¤šä¸ªç›¸å…³æŸ¥è¯¢ï¼‰ç­‰ã€‚è¿™äº›ç­–ç•¥å¯ä»¥æ ¹æ®å…·ä½“åœºæ™¯ç»„åˆä½¿ç”¨ã€‚

å‘é‡æ•°æ®åº“çš„é€‰æ‹©ä¹Ÿä¼šå½±å“æ£€ç´¢æ•ˆæœã€‚FAISS é€‚åˆå°è§„æ¨¡æ•°æ®å’Œç²¾ç¡®æ£€ç´¢åœºæ™¯ã€‚å¯¹äºæ›´å¤§è§„æ¨¡çš„æ•°æ®ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ Milvusã€Weaviateã€Chroma ç­‰ä¸“ä¸šçš„å‘é‡æ•°æ®åº“ã€‚è¿™äº›æ•°æ®åº“æä¾›äº†åˆ†å¸ƒå¼æ¶æ„ã€æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ›´ä¸°å¯Œçš„åŠŸèƒ½ã€‚

### 3.3 RAG ç³»ç»Ÿçš„å¸¸è§é—®é¢˜

ä¸Šä¸‹æ–‡çª—å£æº¢å‡ºæ˜¯å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å¸¸è§é—®é¢˜ã€‚å½“æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å¤ªé•¿ï¼Œè¶…è¿‡äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶æ—¶ï¼Œéœ€è¦è¿›è¡Œæˆªæ–­æˆ–å‹ç¼©ã€‚å¸¸ç”¨çš„ç­–ç•¥åŒ…æ‹¬ï¼šé€‰æ‹©æ€§æˆªæ–­ï¼ˆä¿ç•™æœ€ç›¸å…³çš„å†…å®¹ï¼‰ã€æ‘˜è¦å‹ç¼©ï¼ˆä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼‰ã€åˆ†å±‚æ£€ç´¢ï¼ˆå…ˆæ£€ç´¢æ‘˜è¦ï¼Œå†æ£€ç´¢è¯¦ç»†å†…å®¹ï¼‰ã€‚

æ£€ç´¢ç»“æœçš„å¤šæ ·æ€§ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç›¸ä¼¼åº¦æœ€é«˜çš„å‡ ä¸ªæ–‡æ¡£å¾€å¾€å†…å®¹ç›¸ä¼¼ï¼Œæ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯ã€‚MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰æ£€ç´¢æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå®ƒåœ¨ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè¿”å›çš„å†…å®¹æ—¢ç›¸å…³åˆä¸é‡å¤ã€‚

å†·å¯åŠ¨é—®é¢˜æ˜¯æ–°ç”¨æˆ·æˆ–æ–°æ–‡æ¡£åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ã€‚å¯¹äºæ–°ç”¨æˆ·ï¼Œç³»ç»Ÿæ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®æ¥ç†è§£ç”¨æˆ·æ„å›¾ï¼›å¯¹äºæ–°æ–‡æ¡£ï¼Œå‘é‡ç´¢å¼•ä¸­å¯èƒ½æ²¡æœ‰è¶³å¤Ÿçš„ç›¸å…³å†…å®¹ã€‚è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ï¼šä½¿ç”¨ç”¨æˆ·ç”»åƒå’Œæ¨èç³»ç»Ÿæ”¹å–„ç”¨æˆ·ä½“éªŒï¼›é‡‡ç”¨æ¸è¿›å¼ç´¢å¼•ç­–ç•¥ï¼Œæ–°æ–‡æ¡£é€æ­¥åŠ å…¥ç´¢å¼•ã€‚

## è¯¾åä½œä¸š

### åŸºç¡€ä½œä¸š

**ä½œä¸š 1ï¼šå¤šæ ¼å¼æ–‡æ¡£åŠ è½½å™¨**

æ‰©å±•æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼ã€‚è¦æ±‚ï¼š
- å®ç° Word æ–‡æ¡£ï¼ˆ.docxï¼‰åŠ è½½åŠŸèƒ½
- å®ç° Excel æ–‡ä»¶ï¼ˆ.xlsxï¼‰åŠ è½½åŠŸèƒ½
- å®ç° CSV æ–‡ä»¶åŠ è½½åŠŸèƒ½
- ä¸ºæ¯ç§æ ¼å¼è®¾è®¡åˆé€‚çš„å…ƒæ•°æ®æå–ç­–ç•¥
- ç¼–å†™æµ‹è¯•ç”¨ä¾‹éªŒè¯å„ç§æ ¼å¼çš„åŠ è½½æ•ˆæœ

**ä½œä¸š 2ï¼šæ™ºèƒ½åˆ†å—ç­–ç•¥**

å®ç°è‡ªå®šä¹‰çš„åˆ†å—ç­–ç•¥ã€‚è¦æ±‚ï¼š
- å®ç°æŒ‰æ®µè½åˆ†å—ï¼ˆä¿æŒæ®µè½å®Œæ•´æ€§ï¼‰
- å®ç°æŒ‰å¥å­åˆ†å—ï¼ˆé€‚åˆçŸ­æ–‡æœ¬ï¼‰
- å®ç°è¯­ä¹‰åˆ†å—ï¼ˆä½¿ç”¨æ¨¡å‹åˆ¤æ–­è¯­ä¹‰è¾¹ç•Œï¼‰
- æ¯”è¾ƒä¸åŒç­–ç•¥çš„æ£€ç´¢æ•ˆæœ

### ä¸­çº§ä½œä¸š

**ä½œä¸š 3ï¼šæ··åˆæ£€ç´¢ç³»ç»Ÿ**

æ„å»ºæ··åˆæ£€ç´¢ç³»ç»Ÿã€‚è¦æ±‚ï¼š
- å®ç°å…³é”®è¯æ£€ç´¢ï¼ˆä½¿ç”¨ BM25 æˆ– TF-IDFï¼‰
- å®ç°å‘é‡æ£€ç´¢ï¼ˆä½¿ç”¨ FAISSï¼‰
- å®ç°ç»“æœèåˆç­–ç•¥
- è¯„ä¼°æ··åˆæ£€ç´¢ vs å•ä¸€æ£€ç´¢çš„æ•ˆæœå·®å¼‚

**ä½œä¸š 4ï¼šæ£€ç´¢ç»“æœé‡æ’åº**

å®ç°æ£€ç´¢ç»“æœé‡æ’åºåŠŸèƒ½ã€‚è¦æ±‚ï¼š
- ä½¿ç”¨äº¤å‰ç¼–ç å™¨ï¼ˆCross-Encoderï¼‰è¿›è¡Œé‡æ’åº
- å®ç°æ‰¹é‡é‡æ’åºä¼˜åŒ–
- æ¯”è¾ƒé‡æ’åºå‰åçš„æ•ˆæœå·®å¼‚
- åˆ†æé‡æ’åºçš„è®¡ç®—æˆæœ¬

### é«˜çº§ä½œä¸š

**ä½œä¸š 5ï¼šå¯é…ç½®çš„ RAG æ¡†æ¶**

è®¾è®¡ä¸€ä¸ªå¯é…ç½®çš„ RAG æ¡†æ¶ã€‚è¦æ±‚ï¼š
- æ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢åˆ†å—ç­–ç•¥
- æ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢æ£€ç´¢å™¨ç±»å‹
- æ”¯æŒè‡ªå®šä¹‰æ£€ç´¢æµç¨‹ï¼ˆå•è½®/å¤šè½®/è¿­ä»£ï¼‰
- æä¾› Web é…ç½®ç•Œé¢

**ä½œä¸š 6ï¼šå¢é‡ç´¢å¼•ç³»ç»Ÿ**

å®ç°å¢é‡ç´¢å¼•æ›´æ–°ç³»ç»Ÿã€‚è¦æ±‚ï¼š
- æ”¯æŒæ–‡æ¡£çš„å¢åˆ æ”¹æŸ¥
- å®ç°å¢é‡å‘é‡åŒ–ï¼ˆåªå¤„ç†å˜åŒ–çš„æ–‡æ¡£ï¼‰
- æ”¯æŒç´¢å¼•ç‰ˆæœ¬ç®¡ç†
- æä¾›ç´¢å¼•å¤‡ä»½å’Œæ¢å¤åŠŸèƒ½

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šé«˜çº§æ£€ç´¢å™¨é…ç½®

```python
# rag/advanced_retrievers.py
"""
é«˜çº§æ£€ç´¢å™¨é…ç½®
æä¾›å¤šç§æ£€ç´¢ç­–ç•¥çš„å®ç°
"""

from typing import List, Dict, Any
from langchain_core.retrievers import EnsembleRetriever
from langchain_core.vectorstores import VectorStore
from langchain_core.documents import Document
from rag.retrievers import create_retriever
from config import get_logger

logger = get_logger(__name__)


class MMRRetriever:
    """æœ€å¤§è¾¹é™…ç›¸å…³æ€§æ£€ç´¢å™¨"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        k: int = 5,
        fetch_k: int = 20,
        lambda_mult: float = 0.5
    ):
        """
        åˆå§‹åŒ– MMR æ£€ç´¢å™¨
        
        Args:
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹
            k: æœ€ç»ˆè¿”å›çš„æ–‡æ¡£æ•°
            fetch_k: åˆæ­¥æ£€ç´¢çš„å€™é€‰æ–‡æ¡£æ•°
            lambda_mult: æ§åˆ¶ç›¸å…³æ€§å’Œå¤šæ ·æ€§çš„å¹³è¡¡
                        å€¼è¶Šå°è¶Šå€¾å‘äºå¤šæ ·æ€§
        """
        self.vector_store = vector_store
        self.k = k
        self.fetch_k = fetch_k
        self.lambda_mult = lambda_mult
        
        # åˆ›å»ºåº•å±‚æ£€ç´¢å™¨
        self._retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,
                "fetch_k": fetch_k,
                "lambda_mult": lambda_mult
            }
        )
    
    def invoke(self, query: str) -> List[Document]:
        """æ‰§è¡Œ MMR æ£€ç´¢"""
        return self._retriever.invoke(query)
    
    def config_info(self) -> Dict[str, Any]:
        """è·å–é…ç½®ä¿¡æ¯"""
        return {
            "type": "MMR",
            "k": self.k,
            "fetch_k": self.fetch_k,
            "lambda_mult": self.lambda_mult
        }


class ThresholdRetriever:
    """é˜ˆå€¼è¿‡æ»¤æ£€ç´¢å™¨"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        score_threshold: float = 0.7,
        k: int = 10
    ):
        """
        åˆå§‹åŒ–é˜ˆå€¼æ£€ç´¢å™¨
        
        Args:
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            k: æœ€å¤§è¿”å›æ–‡æ¡£æ•°
        """
        self.vector_store = vector_store
        self.score_threshold = score_threshold
        self.k = k
        
        self._retriever = vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "score_threshold": score_threshold,
                "k": k
            }
        )
    
    def invoke(self, query: str) -> List[Document]:
        """æ‰§è¡Œé˜ˆå€¼æ£€ç´¢"""
        return self._retriever.invoke(query)


class EnsembleRetriever:
    """é›†æˆæ£€ç´¢å™¨"""
    
    def __init__(
        self,
        retrievers: List,
        weights: List[float] = None
    ):
        """
        åˆå§‹åŒ–é›†æˆæ£€ç´¢å™¨
        
        Args:
            retrievers: æ£€ç´¢å™¨åˆ—è¡¨
            weights: å„æ£€ç´¢å™¨çš„æƒé‡
        """
        if weights is None:
            weights = [1.0 / len(retrievers)] * len(retrievers)
        
        self.retrievers = retrievers
        self.weights = weights
        
        # åˆ›å»º LangChain é›†æˆæ£€ç´¢å™¨
        self._retriever = EnsembleRetriever(
            retrievers=[r._retriever for r in retrievers],
            weights=weights
        )
    
    def invoke(self, query: str) -> List[Document]:
        """æ‰§è¡Œé›†æˆæ£€ç´¢"""
        return self._retriever.invoke(query)


def create_mmr_retriever(
    vector_store: VectorStore,
    k: int = 5,
    **kwargs
) -> MMRRetriever:
    """åˆ›å»º MMR æ£€ç´¢å™¨çš„ä¾¿æ·å‡½æ•°"""
    return MMRRetriever(
        vector_store=vector_store,
        k=k,
        **kwargs
    )


def create_threshold_retriever(
    vector_store: VectorStore,
    score_threshold: float = 0.7,
    k: int = 10
) -> ThresholdRetriever:
    """åˆ›å»ºé˜ˆå€¼æ£€ç´¢å™¨çš„ä¾¿æ·å‡½æ•°"""
    return ThresholdRetriever(
        vector_store=vector_store,
        score_threshold=score_threshold,
        k=k
    )
```

### ç¤ºä¾‹ 2ï¼šæŸ¥è¯¢å¤„ç†ç®¡é“

```python
# rag/query_pipeline.py
"""
æŸ¥è¯¢å¤„ç†ç®¡é“
å®ç°æŸ¥è¯¢æ”¹å†™ã€æ‰©å±•å’Œåå¤„ç†
"""

from typing import List, Dict, Any
from langchain_core.documents import Document
from config import get_logger

logger = get_logger(__name__)


class QueryProcessor:
    """æŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è¯¢å¤„ç†å™¨"""
        pass
    
    def process(self, query: str) -> str:
        """
        å¤„ç†æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            å¤„ç†åçš„æŸ¥è¯¢
        """
        # å­ç±»å®ç°å…·ä½“å¤„ç†é€»è¾‘
        return query


class QueryRewriter(QueryProcessor):
    """æŸ¥è¯¢æ”¹å†™å™¨"""
    
    def __init__(self, model=None):
        super().__init__()
        self.model = model
    
    def process(self, query: str) -> str:
        """
        æ”¹å†™æŸ¥è¯¢ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            æ”¹å†™åçš„æŸ¥è¯¢
        """
        if self.model is None:
            return query
        
        # ä½¿ç”¨ LLM æ”¹å†™æŸ¥è¯¢
        prompt = f"""å°†ä»¥ä¸‹æŸ¥è¯¢æ”¹å†™ä¸ºæ›´é€‚åˆçŸ¥è¯†åº“æ£€ç´¢çš„å½¢å¼ã€‚
ä¿æŒåŸæ„ï¼Œä½†ä½¿ç”¨æ›´æ­£å¼ã€æ›´å…·ä½“çš„è¡¨è¿°ã€‚

åŸå§‹æŸ¥è¯¢: {query}
æ”¹å†™æŸ¥è¯¢:"""
        
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ”¹å†™æŸ¥è¯¢
        rewritten = self.model.invoke(prompt)
        
        return rewritten.strip()


class QueryExpander(QueryProcessor):
    """æŸ¥è¯¢æ‰©å±•å™¨"""
    
    def __init__(self, model=None, num_expansions: int = 3):
        super().__init__()
        self.model = model
        self.num_expansions = num_expansions
    
    def process(self, query: str) -> List[str]:
        """
        æ‰©å±•æŸ¥è¯¢ä¸ºå¤šä¸ªç›¸å…³æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            æ‰©å±•åçš„æŸ¥è¯¢åˆ—è¡¨
        """
        if self.model is None:
            return [query]
        
        prompt = f"""ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ {self.num_expansions} ä¸ªä¸åŒçš„è¡¨è¿°å½¢å¼ã€‚
æ¯ä¸ªè¡¨è¿°åº”è¯¥ä»ä¸åŒè§’åº¦æˆ–ä½¿ç”¨ä¸åŒè¯æ±‡è¡¨è¾¾ç›¸åŒçš„æŸ¥è¯¢æ„å›¾ã€‚

åŸå§‹æŸ¥è¯¢: {query}

è¡¨è¿°1:
è¡¨è¿°2:
è¡¨è¿°3:"""
        
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        response = self.model.invoke(prompt)
        
        # è§£æå“åº”
        expansions = [
            line.strip()
            for line in response.split('\n')
            if line.strip() and not line.startswith('è¡¨è¿°')
        ]
        
        if not expansions:
            return [query]
        
        return [query] + expansions[:self.num_expansions]


class QueryPipeline:
    """æŸ¥è¯¢å¤„ç†ç®¡é“"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è¯¢ç®¡é“"""
        self.processors: List[QueryProcessor] = []
    
    def add_processor(self, processor: QueryProcessor):
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)
    
    def process(self, query: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æŸ¥è¯¢å¤„ç†æµç¨‹
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        result = {
            "original_query": query,
            "processed_queries": [query],
            "steps": []
        }
        
        current_query = query
        
        for processor in self.processors:
            step_result = {
                "processor": processor.__class__.__name__,
                "input": current_query
            }
            
            if isinstance(processor, QueryExpander):
                expansions = processor.process(current_query)
                current_query = expansions[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºä¸»æŸ¥è¯¢
                result["processed_queries"] = expansions
                step_result["output"] = expansions
            else:
                current_query = processor.process(current_query)
                step_result["output"] = current_query
            
            result["steps"].append(step_result)
        
        result["final_query"] = current_query
        
        return result


def create_query_pipeline(use_rewriting: bool = True, use_expansion: bool = False):
    """
    åˆ›å»ºæŸ¥è¯¢ç®¡é“çš„ä¾¿æ·å‡½æ•°
    
    Args:
        use_rewriting: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ”¹å†™
        use_expansion: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
        
    Returns:
        é…ç½®å¥½çš„æŸ¥è¯¢ç®¡é“
    """
    from langchain_openai import ChatOpenAI
    from config import settings
    
    pipeline = QueryPipeline()
    
    if use_rewriting:
        model = ChatOpenAI(model=settings.openai_model)
        pipeline.add_processor(QueryRewriter(model))
    
    if use_expansion:
        if 'model' not in locals():
            model = ChatOpenAI(model=settings.openai_model)
        pipeline.add_processor(QueryExpander(model))
    
    return pipeline
```

## å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

- LangChain RAG æ–‡æ¡£ï¼šhttps://docs.langchain.com/oss/python/langchain/text_splitters
- FAISS å®˜æ–¹æ–‡æ¡£ï¼šhttps://github.com/facebookresearch/faiss
- OpenAI Embeddings æ–‡æ¡£ï¼šhttps://platform.openai.com/docs/guides/embeddings

### æŠ€æœ¯è®ºæ–‡

- Retrieval-Augmented Generation åŸå§‹è®ºæ–‡ï¼šhttps://arxiv.org/abs/2005.11401
- BGE Embedding è®ºæ–‡ï¼šhttps://arxiv.org/abs/2309.16541
- RAG è¯„ä¼°åŸºå‡†ï¼šhttps://github.com/Retrieval-Enhanced-Generative-Process/benchmark

### è¿›é˜¶èµ„æº

- Awesome RAG èµ„æºé›†åˆï¼šhttps://github.com/huggingface/awesome-rag
- LangChain RAG æ•™ç¨‹ï¼šhttps://github.com/langchain-ai/rag-chunking
- å‘é‡æ•°æ®åº“æ¯”è¾ƒï¼šhttps://github.com/vecs xyz/vector-db-benchmark
